{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b69c618e",
   "metadata": {},
   "source": [
    "# Ticketmaster Two‑Stage Pricing Modeling (Interview‑Ready)\n",
    "\n",
    "This notebook builds an **interview‑ready, end‑to‑end** modeling pipeline on `events_history.parquet`.\n",
    "\n",
    "**Goal:** Model ticket pricing behavior using a **two‑stage approach**:\n",
    "1. **Stage 1 (Classification):** predict whether an event snapshot has a *usable* price (`has_price`).\n",
    "2. **Stage 2 (Regression):** predict `min_price` *conditional on* `has_price = 1`.\n",
    "3. Combine into **Expected Price**:  \\(E[price] = P(has\\_price) \\times \\hat{price}\\)\n",
    "\n",
    "Key design choices (worth mentioning in an interview):\n",
    "- **Group split by `id`** to avoid leakage from multiple snapshots of the same event.\n",
    "- Robust handling of **timezone‑aware vs timezone‑naive** timestamps.\n",
    "- Evaluation focuses on **ROC‑AUC / PR‑AUC** for Stage 1 and **MAE / RMSE / MedAE** for Stage 2.\n",
    "- Interpretable “variable importance” via **logistic regression coefficients (odds ratios)** and **permutation importance**.\n",
    "\n",
    "> Tip: If your notebook lives in `src/`, paths are resolved relative to `src/`. This notebook computes project root automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e05f46",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c40283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    mean_absolute_error, mean_squared_error, median_absolute_error\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadb0b3",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196061fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust if needed\n",
    "PROJECT_ROOT = Path.cwd().parent  # notebook in src/\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"events_history.parquet\"\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b0bc9",
   "metadata": {},
   "source": [
    "## 2. Basic cleaning & feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2efbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Parse datetimes consistently: parse as UTC, then drop tz so arithmetic works\n",
    "date_cols = [\"date\", \"onsale_date\", \"offsale_date\", \"snapshot_date\"]\n",
    "for c in date_cols:\n",
    "    df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "\n",
    "# Extract hour if time looks like HH:MM:SS (otherwise becomes NaN)\n",
    "df[\"event_hour\"] = pd.to_datetime(df[\"time\"], format=\"%H:%M:%S\", errors=\"coerce\").dt.hour\n",
    "\n",
    "# Relative-time features (in days)\n",
    "df[\"days_until_event\"] = (df[\"date\"] - df[\"snapshot_date\"]).dt.total_seconds() / 86400\n",
    "df[\"days_since_onsale\"] = (df[\"snapshot_date\"] - df[\"onsale_date\"]).dt.total_seconds() / 86400\n",
    "df[\"days_until_offsale\"] = (df[\"offsale_date\"] - df[\"snapshot_date\"]).dt.total_seconds() / 86400\n",
    "\n",
    "# Calendar features\n",
    "df[\"event_dow\"] = df[\"date\"].dt.dayofweek  # 0=Mon\n",
    "df[\"event_month\"] = df[\"date\"].dt.month\n",
    "\n",
    "# Light clipping to keep extreme values from dominating baselines\n",
    "for c in [\"days_until_event\", \"days_since_onsale\", \"days_until_offsale\"]:\n",
    "    df[c] = df[c].clip(-3650, 3650)\n",
    "\n",
    "df[[\"date\",\"snapshot_date\",\"onsale_date\",\"offsale_date\",\"event_hour\",\"days_until_event\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16117aed",
   "metadata": {},
   "source": [
    "## 3. Define two‑stage targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure numeric\n",
    "df[\"min_price\"] = pd.to_numeric(df[\"min_price\"], errors=\"coerce\")\n",
    "\n",
    "# Define whether a usable price exists.\n",
    "# NOTE: If your domain interpretation is that 0 means 'free', you may choose >=0 instead.\n",
    "df[\"has_price\"] = df[\"min_price\"].notna() & (df[\"min_price\"] > 0)\n",
    "\n",
    "# Log transform for skewed prices (regression stage)\n",
    "df[\"log_min_price\"] = np.log1p(df[\"min_price\"])\n",
    "\n",
    "df[\"has_price\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6cf14f",
   "metadata": {},
   "source": [
    "## 4. Train/test split by event id (prevents snapshot leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fd9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_COL = \"id\"\n",
    "\n",
    "# Baseline feature set (drop obvious leakage and high-cardinality text for now)\n",
    "DROP_COLS = [\n",
    "    \"min_price\", \"log_min_price\", \"has_price\",\n",
    "    \"max_price\",            # avoid leaking min_price from max_price for this baseline\n",
    "    \"url\", \"name\"           # high-cardinality text; can add later with TF-IDF if desired\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in DROP_COLS]\n",
    "X = df[feature_cols].copy()\n",
    "y_cls = df[\"has_price\"].astype(int)\n",
    "y_reg = df[\"log_min_price\"]\n",
    "groups = df[GROUP_COL]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y_cls, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_cls_train, y_cls_test = y_cls.iloc[train_idx], y_cls.iloc[test_idx]\n",
    "y_reg_train, y_reg_test = y_reg.iloc[train_idx], y_reg.iloc[test_idx]\n",
    "\n",
    "train_priced = (y_cls_train == 1)\n",
    "test_priced = (y_cls_test == 1)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train priced rate:\", y_cls_train.mean(), \"Test priced rate:\", y_cls_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388067ce",
   "metadata": {},
   "source": [
    "## 5. Preprocessing (numeric vs categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d457780",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = [c for c in X_train.columns if c not in numeric_features]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    # Dense output to support models that don't accept sparse matrices (e.g., HistGradientBoosting)\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=50, sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2947e",
   "metadata": {},
   "source": [
    "## 6. Stage 1: compare multiple classifiers (has_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_stage1(model, X_train, y_train, X_test, y_test, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"roc_auc\": roc_auc_score(y_test, proba),\n",
    "        \"pr_auc\": average_precision_score(y_test, proba)\n",
    "    }\n",
    "\n",
    "clf_lr = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, solver=\"saga\", class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "clf_hgb = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", HistGradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "clf_rf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300, random_state=42, n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\", min_samples_leaf=5\n",
    "    ))\n",
    "])\n",
    "\n",
    "results_stage1 = []\n",
    "results_stage1.append(eval_stage1(clf_lr,  X_train, y_cls_train, X_test, y_cls_test, \"LogReg (saga, balanced)\"))\n",
    "results_stage1.append(eval_stage1(clf_hgb, X_train, y_cls_train, X_test, y_cls_test, \"HistGBClassifier\"))\n",
    "results_stage1.append(eval_stage1(clf_rf,  X_train, y_cls_train, X_test, y_cls_test, \"RandomForestClassifier\"))\n",
    "\n",
    "stage1_df = pd.DataFrame(results_stage1).sort_values(\"roc_auc\", ascending=False)\n",
    "stage1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8eb97b",
   "metadata": {},
   "source": [
    "### Optional: quick GroupKFold CV for Stage 1 (LogReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional. Comment out if you want faster runtime.\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "cv_scores = cross_validate(\n",
    "    clf_lr,\n",
    "    X, y_cls,\n",
    "    groups=groups,\n",
    "    cv=gkf,\n",
    "    scoring={\"roc_auc\":\"roc_auc\", \"pr_auc\":\"average_precision\"},\n",
    "    n_jobs=-1\n",
    ")\n",
    "pd.DataFrame({\n",
    "    \"roc_auc\": cv_scores[\"test_roc_auc\"],\n",
    "    \"pr_auc\": cv_scores[\"test_pr_auc\"]\n",
    "}).agg([\"mean\",\"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb151041",
   "metadata": {},
   "source": [
    "## 7. Stage 1: pick best model and tune threshold (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best Stage 1 model by ROC-AUC (you can choose PR-AUC instead if positives are rare)\n",
    "best_stage1_name = stage1_df.iloc[0][\"model\"]\n",
    "best_stage1 = {\"LogReg (saga, balanced)\": clf_lr,\n",
    "               \"HistGBClassifier\": clf_hgb,\n",
    "               \"RandomForestClassifier\": clf_rf}[best_stage1_name]\n",
    "\n",
    "best_stage1.fit(X_train, y_cls_train)\n",
    "p_has_price = best_stage1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Best Stage 1:\", best_stage1_name)\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_cls_test, p_has_price))\n",
    "print(\"PR-AUC :\", average_precision_score(y_cls_test, p_has_price))\n",
    "\n",
    "# Example capacity-based threshold: flag top 10% as \"priced\"\n",
    "threshold = float(np.quantile(p_has_price, 0.90))\n",
    "y_pred = (p_has_price >= threshold).astype(int)\n",
    "\n",
    "tp = int(((y_pred==1) & (y_cls_test==1)).sum())\n",
    "fp = int(((y_pred==1) & (y_cls_test==0)).sum())\n",
    "fn = int(((y_pred==0) & (y_cls_test==1)).sum())\n",
    "tn = int(((y_pred==0) & (y_cls_test==0)).sum())\n",
    "\n",
    "precision = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "recall = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "print(f\"Threshold (top 10%): {threshold:.3f} | Precision={precision:.3f} | Recall={recall:.3f} | TP={tp}, FP={fp}, FN={fn}, TN={tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d3e57",
   "metadata": {},
   "source": [
    "## 8. Stage 1 interpretability: odds ratios + permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e75b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds ratios only make sense for logistic regression; if best model isn't LR, fit LR for interpretation\n",
    "clf_lr.fit(X_train, y_cls_train)\n",
    "logreg = clf_lr.named_steps[\"model\"]\n",
    "pre = clf_lr.named_steps[\"preprocess\"]\n",
    "\n",
    "# Build feature names manually from fitted transformers (robust)\n",
    "num_cols = pre.transformers_[0][2]\n",
    "cat_pipe = pre.transformers_[1][1]\n",
    "cat_cols = pre.transformers_[1][2]\n",
    "ohe = cat_pipe.named_steps[\"onehot\"]\n",
    "cat_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "feature_names = np.concatenate([np.array(num_cols, dtype=str), cat_feature_names])\n",
    "\n",
    "coef_df = pd.DataFrame({\"feature\": feature_names, \"coef\": logreg.coef_[0]})\n",
    "coef_df[\"odds_ratio\"] = np.exp(coef_df[\"coef\"])\n",
    "coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
    "\n",
    "coef_df.sort_values(\"abs_coef\", ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4219de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate one-hot importance back to original \"base feature\"\n",
    "def base_feature_name(feat: str) -> str:\n",
    "    return feat.split(\"_\", 1)[0] if \"_\" in feat else feat\n",
    "\n",
    "coef_df[\"base_feature\"] = coef_df[\"feature\"].map(base_feature_name)\n",
    "agg_importance = (coef_df.groupby(\"base_feature\")[\"abs_coef\"].sum().sort_values(ascending=False))\n",
    "agg_importance.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d851e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance on Stage 1 (use the best model; can be slower)\n",
    "perm = permutation_importance(\n",
    "    best_stage1,\n",
    "    X_test, y_cls_test,\n",
    "    n_repeats=8,\n",
    "    random_state=42,\n",
    "    scoring=\"roc_auc\"\n",
    ")\n",
    "\n",
    "perm_df = pd.DataFrame({\n",
    "    \"feature\": best_stage1.named_steps[\"preprocess\"].get_feature_names_out(),\n",
    "    \"importance\": perm.importances_mean\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "perm_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89b87d",
   "metadata": {},
   "source": [
    "## 9. Stage 2: compare multiple regressors (price | priced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f158e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: predict median log-price from train priced events\n",
    "baseline_log = float(np.median(y_reg_train.loc[train_priced]))\n",
    "baseline_pred_log = np.full(test_priced.sum(), baseline_log)\n",
    "baseline_pred = np.expm1(baseline_pred_log)\n",
    "\n",
    "true_price_priced = df.loc[X_test.index[test_priced], \"min_price\"].values\n",
    "\n",
    "def eval_stage2(pred_price, true_price, name):\n",
    "    mae = mean_absolute_error(true_price, pred_price)\n",
    "    rmse = mean_squared_error(true_price, pred_price, squared=False)\n",
    "    medae = median_absolute_error(true_price, pred_price)\n",
    "    return {\"model\": name, \"MAE\": mae, \"RMSE\": rmse, \"MedAE\": medae}\n",
    "\n",
    "stage2_results = [eval_stage2(baseline_pred, true_price_priced, \"Baseline (median)\")]\n",
    "\n",
    "# Ridge regression on log-price (needs dense after preprocess -> we already output dense)\n",
    "reg_ridge = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "reg_hgb = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", HistGradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "reg_rf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=400, random_state=42, n_jobs=-1,\n",
    "        min_samples_leaf=5\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit/predict each on priced subset\n",
    "for mdl, name in [(reg_ridge, \"Ridge (log-price)\"),\n",
    "                  (reg_hgb, \"HistGBRegressor (log-price)\"),\n",
    "                  (reg_rf, \"RandomForestRegressor (log-price)\")]:\n",
    "\n",
    "    mdl.fit(X_train.loc[train_priced], y_reg_train.loc[train_priced])\n",
    "    log_pred = mdl.predict(X_test.loc[test_priced])\n",
    "    pred_price = np.expm1(log_pred)\n",
    "    stage2_results.append(eval_stage2(pred_price, true_price_priced, name))\n",
    "\n",
    "stage2_df = pd.DataFrame(stage2_results).sort_values(\"MAE\")\n",
    "stage2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb1739",
   "metadata": {},
   "source": [
    "## 10. Two-stage expected price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best stage2 by MAE (or MedAE)\n",
    "best_stage2_name = stage2_df.iloc[0][\"model\"]\n",
    "best_stage2 = {\n",
    "    \"Baseline (median)\": None,\n",
    "    \"Ridge (log-price)\": reg_ridge,\n",
    "    \"HistGBRegressor (log-price)\": reg_hgb,\n",
    "    \"RandomForestRegressor (log-price)\": reg_rf\n",
    "}[best_stage2_name]\n",
    "\n",
    "# Stage 1 probabilities\n",
    "p_has_price = best_stage1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Stage 2 predictions for all rows (needed for expected value). For baseline, use constant.\n",
    "if best_stage2 is None:\n",
    "    pred_log_all = np.full(len(X_test), baseline_log)\n",
    "else:\n",
    "    best_stage2.fit(X_train.loc[train_priced], y_reg_train.loc[train_priced])\n",
    "    pred_log_all = best_stage2.predict(X_test)\n",
    "\n",
    "pred_price_all = np.expm1(pred_log_all)\n",
    "expected_price = p_has_price * pred_price_all\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"p_has_price\": p_has_price,\n",
    "    \"pred_price_if_priced\": pred_price_all,\n",
    "    \"expected_price\": expected_price,\n",
    "    \"true_min_price\": df.loc[X_test.index, \"min_price\"].values,\n",
    "    \"true_has_price\": y_cls_test.values,\n",
    "    \"snapshot_date\": df.loc[X_test.index, \"snapshot_date\"].values,\n",
    "    \"event_date\": df.loc[X_test.index, \"date\"].values,\n",
    "    \"city\": df.loc[X_test.index, \"city\"].values,\n",
    "    \"venue\": df.loc[X_test.index, \"venue\"].values,\n",
    "    \"artist\": df.loc[X_test.index, \"artist\"].values,\n",
    "    \"genre\": df.loc[X_test.index, \"genre\"].values,\n",
    "})\n",
    "\n",
    "out.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1568d",
   "metadata": {},
   "source": [
    "## 11. Conclusions (talk-track)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4082a0",
   "metadata": {},
   "source": [
    "**What worked well**\n",
    "- A two-stage design fits the marketplace reality: **availability** and **magnitude** are different problems.\n",
    "- Group splitting by `id` prevents leakage from repeated snapshots of the same event.\n",
    "\n",
    "**Key takeaways**\n",
    "- Stage 1 performance is often very strong because “price exists” is driven by **structural factors** (venue/city/artist/genre) and lifecycle timing.\n",
    "- Stage 2 errors are typically dominated by a small number of extreme prices, so **MAE/MedAE** are more stable than RMSE.\n",
    "\n",
    "**Next steps you can mention**\n",
    "- Add text features from `name` (TF‑IDF) or create “artist popularity” features from historical counts.\n",
    "- Calibrate Stage 1 probabilities and tune thresholds to business capacity (top‑K workflow).\n",
    "- For Stage 2, model quantiles (e.g., pinball loss) or bucket prices (classification) for robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
